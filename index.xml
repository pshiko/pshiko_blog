<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pshikolog</title><link>https://pshiko.github.io/pshikolog/</link><description>Recent content on pshikolog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Mon, 05 Oct 2020 02:00:27 +0900</lastBuildDate><atom:link href="https://pshiko.github.io/pshikolog/index.xml" rel="self" type="application/rss+xml"/><item><title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title><link>https://pshiko.github.io/pshikolog/posts/vit/</link><pubDate>Mon, 05 Oct 2020 02:00:27 +0900</pubDate><guid>https://pshiko.github.io/pshikolog/posts/vit/</guid><description>https://openreview.net/pdf?id=YicbFdNTTy
ICLR2021向けのBlind SubmissionでOpen Review中の論文
Blind Submissionとは言うが, TPU v3と JFT-300Mを利用している時点でGのオーラが隠せていない論文
ざっくり内容 概要 imageを固定pxサイズに分割したパッチにvisual wordsとし、image全体をパッチのシーケンスとして、Transformerに入力するモデル(Vision Transofmer)を提案。 このViTをJFT-300Mのような大規模データセットでpretrainすることで、downstream taskにおいてBigTransfer(ResNetベース)やEfficientNetより低い計算コスト(推論時スループット及び訓練時間の観点)で同等以上の精度が達成できることを確認した 新規性 完全にCNNを排除したシンプルなTransformerのアーキテクチャでCNNベースのモデルを超える精度が出たのは初 ViTにおいて、ImageNetではデータ数が足りないが大規模データセットによるpretrainによってCNNのような局所性やシフトインバリアントな特徴をアーキテクチャに組み込まなくても学習ができることを明らかにした点 手法 基本的にはSimpleなTransformer
固定のパッチサイズに画像を分割 Position Embeddingは固定ではなく学習ベース 特徴的な点は以下の通り
シーケンスの先頭にはClassトークンを入れる。 classトークンのネットワークの出力で画像ラベルを分類するSupervised Taskで学習をする(Masked LMのようなSelf-supervisedでの学習はしない) 解像度の異なる画像を入力とする際はパッチサイズは固定のまま、Position Encodingの値を線形補間で与える。 Conifgurationはこんな感じ
入力を生の画像ではなくResNetをbackboneとしたFeatureMapを入力としたHybirdModelも実験
実験 ImageNetやJFT-300Mを組み合わせた大規模データセットでモデルをpretrained
BigTransfer(ResNet152x4)やNoisyStudent(EfficientNet-L2)とかと各種down stream taskで比較
Noisy Studentに匹敵する精度を達成しながら、Pretrainにかかる時間は他のモデルより大分抑えられている(とはいえこのTPUv3-days 2.5kってどんなパワーやねん)
pretrainのデータセットのサイズを変化させながら比較
JFT-300M位までデータセットを拡張すればViTの効果が出てくる感
JFT-300Mでpretrainした場合のcomputation cost</description></item><item><title>ADER: Adaptively Distilled Exemplar Replay towards Continual Learning for Session-based Recommendation</title><link>https://pshiko.github.io/pshikolog/posts/ader/</link><pubDate>Mon, 05 Oct 2020 00:49:41 +0900</pubDate><guid>https://pshiko.github.io/pshikolog/posts/ader/</guid><description>https://dl.acm.org/doi/abs/10.1145/3383313.3412218
ざっくり内容 概要 Session based recommendation において、継続的に取得されるデータに対して、モデルの破壊的忘却を防ぎながら継続的に学習させ続ける continual learning の手法を提案した研究 新規性 Session based recommendation における Continual Learning という課題設定 手法 モデルの破壊的忘却を防ぐために, 以下の 3 つの要素を導入 過去の学習タイミングで取得できたデータの一部を replay する Exampler
サンプルするデータ数は,クラス(ラベル)毎の出現率に比例して割当て サンプリングアルゴリズムは herding technique を使う(incremental learningとかではよく使うみたい？) クラス内の中心ベクトルに近いデータをサンプルする奴 予測結果の変化に対して制約を設ける Distilation Loss の追加
Distilation Loss と通常の CE Loss の重みをデータサイズに合わせて adaptive に変化
新規データDとExamplerのEの比 1step前と今回のアイテムカテゴリ数(ラベル数)の比 有効性の検証 ベースの Self Attentive Sequential Recomendation(SASrec) に対して上記手法を適応し、 複数のデータセットで Drop Out などの破壊的忘却を防ぐ手法より高い効果があることを確認 継続的学習ではなく、その時点で取得できる全データを利用して学習する手法 雑感 結構学習時にデータセットの期間ってどうしようかなみたいなのは悩みポイントの１つなので割と実用的な課題設定(さすがRecSys2020 の Best Short Paper)</description></item></channel></rss>